## **Week 1**


**Big Data** : If a single device is not enough to process data.

**Storage Solutions** :
  
  **ApacheSpark** : Break Files into smaller chunks and process it on different machines.  (Parallelization) 
  
JBoD : Just a bunch of disk . Link :
HDFS : Link :
RDD : Link :
DataFrame Api vs  Sql vs RDD

Why Use ApacheSpark?

## **Week 2**

Sampling: Reduces data size while preserving statistical properties

Why median ? more outlier resistant.

n = len(list)

mean (mu) = sum(list)/ n


error = x-mu

variance = sigma( (x-mu)^2)/n

std = sqrt(var)

cov = sigma( error1 * error2 )/n

skewness = error^3/(n*std^3)

kurtosis = error^4/ (n * std^4)

PCA : [link]()

Week 3:
Pipelining and unsupervised Learning

Week 4:
Supervised Learning
