## **Week 1**


**Big Data** : If a single device is not enough to process data.

**Storage Solutions** :
  Type             | Cost | Scalability | Speed | Flexibility  | \
  Sql              | H    |      L      |  H    |    L         |     
  NoSql            | H    |      L      |  H    |    L         | \
  ObjectStorage    | H    |      L      |  H    |    L         | 
  
  **ApachSpark** : Break Files into smaller chunks and process it on different machines.  (Parallelization) 
  
JBoD : Just a bunch of disk . Link :
HDFS : Link :
RDD : Link :
DataFrame Api vs  Sql vs RDD

Why Use ApacheSpark?

## **Week 2**

Sampling: Reduces data size while preserving statistical properties

Why median ? more outlier resistant.

n = len(list)

mean (mu) = sum(list)/ n

error = x-mu

variance = sigma( (x-mu)^2)/n

std = sqrt(var)

cov = sigma( error1 * error2 )/n

skewness = error^3/(n*std^3)

kurtosis = error^4/ (n * std^4)

PCA : [link]()
